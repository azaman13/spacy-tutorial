{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import spacy and English models\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> This is the original sentence:\n",
      "Hello, world. Natural Language Processing in 10 lines of code.\n",
      "\n",
      "\n",
      "=> This is the first token:\n",
      "Hello\n",
      "\n",
      "=> Printing sentences (one sentence per line)\n",
      "Hello, world.\n",
      "Natural Language Processing in 10 lines of code.\n"
     ]
    }
   ],
   "source": [
    "# Process the sentences \n",
    "print(\"=> This is the original sentence:\")\n",
    "print(\"Hello, world. Natural Language Processing in 10 lines of code.\\n\")\n",
    "\n",
    "doc = nlp(u\"Hello, world. Natural Language Processing in 10 lines of code.\")\n",
    "\n",
    "\n",
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(\"\\n=> This is the first token:\")\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "print(\"\\n=> Printing sentences (one sentence per line)\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********** Part of Speech tagging ***********\n",
      "\n",
      "Hello - INTJ\n",
      ", - PUNCT\n",
      "world - NOUN\n",
      ". - PUNCT\n",
      "Natural - PROPN\n",
      "Language - PROPN\n",
      "Processing - PROPN\n",
      "in - ADP\n",
      "10 - NUM\n",
      "lines - NOUN\n",
      "of - ADP\n",
      "code - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n*********** Part of Speech tagging ***********\\n\")\n",
    "# Part of Speech taggin*\n",
    "\n",
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********** Named Entities Example ***********\n",
      "\n",
      "Paris - GPE\n",
      "Jack - PERSON\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n*********** Named Entities Example ***********\\n\")\n",
    "\n",
    "doc_2 = nlp(u\"I went to Paris where I met my old friend Jack from uni.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** \n",
      " Print noun chunks for: I went to Paris where I met my old friend Jack from uni.\n",
      "****\n",
      "[I, Paris, I, my old friend, uni]\n"
     ]
    }
   ],
   "source": [
    "print(\"*** \\n Print noun chunks for: I went to Paris where I met my old friend Jack from uni.\\n****\")\n",
    "# Print noun chunks for doc_2\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***\n",
      " For every token in doc_2, print log-probability of the word, \n",
      "estimated from counts from a large corpus\n",
      "****\n",
      "\n",
      "(I, '=>', -4.064180850982666)\n",
      "(went, '=>', -8.474893569946289)\n",
      "(to, '=>', -3.83851957321167)\n",
      "(Paris, '=>', -11.6917724609375)\n",
      "(where, '=>', -7.183883190155029)\n",
      "(I, '=>', -4.064180850982666)\n",
      "(met, '=>', -9.784490585327148)\n",
      "(my, '=>', -5.918124675750732)\n",
      "(old, '=>', -7.7954816818237305)\n",
      "(friend, '=>', -8.825821876525879)\n",
      "(Jack, '=>', -11.20296573638916)\n",
      "(from, '=>', -6.028810501098633)\n",
      "(uni, '=>', -19.579313278198242)\n",
      "(., '=>', -3.0729479789733887)\n"
     ]
    }
   ],
   "source": [
    "# Unigram Probabilities based on a large corpus from spacy\n",
    "\n",
    "print(\"\\n***\\n For every token in doc_2, print log-probability of the word, \\nestimated from counts from a large corpus\\n****\\n\")\n",
    "# The probability estimate is based on counts from a 3 billion word\n",
    "# corpus, smoothed using the Simple Good-Turing method.\n",
    "for token in doc_2:\n",
    "    print(token, '=>', token.prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **** Word embedding based Similarity  ****  \n",
      "\n",
      "Example 1: The King ordered the man to remove the woman and kill his queen\n",
      "('Similarity between king and man:', 0.40884606921875327)\n",
      "('Similarity between king and woman:', 0.26556595924212256)\n",
      "('Similarity between king and queen:', 0.23910408661703836)\n",
      "\n",
      " \n",
      "Example 2: The King ordered the man to remove the woman and kill his queen.\n",
      "('Apples and oranges similarity score:', 0.0)\n",
      "('Boots and hippos similarity score:', 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n **** Word embedding based Similarity  ****  \\n\")\n",
    "\n",
    "print(\"Example 1: The King ordered the man to remove the woman and kill his queen\")\n",
    "\n",
    "doc3 = nlp(u\"The King ordered the man to remove the woman and kill his queen.\")\n",
    "king = doc3[1]\n",
    "man = doc3[4]\n",
    "woman = doc3[8]\n",
    "queen = doc3[-1]\n",
    "print(\"Similarity between king and man:\",king.similarity(man))\n",
    "\n",
    "print(\"Similarity between king and woman:\",king.similarity(woman))\n",
    "\n",
    "print(\"Similarity between king and queen:\",king.similarity(queen))\n",
    "\n",
    "\n",
    "print(\"\\n \\nExample 2: The King ordered the man to remove the woman and kill his queen.\")\n",
    "\n",
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc4 = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc4[0]\n",
    "oranges = doc4[2]\n",
    "boots = doc4[6]\n",
    "hippos = doc4[8]\n",
    "print(\"Apples and oranges similarity score:\", apples.similarity(oranges))\n",
    "print(\"Boots and hippos similarity score:\", boots.similarity(hippos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " We can also do: Similarity between sentence and a word.\n",
      "\n",
      "Similarity between the apple's sentence and the word \n",
      "0.569403188405\n",
      "0.323890854232\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n We can also do: Similarity between sentence and a word.\\n\")\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc4.sents\n",
    "fruit = doc4.vocab[u'fruit']\n",
    "print(\"Similarity between the apple's sentence and the word \")\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-class-env",
   "language": "python",
   "name": "health-class-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
